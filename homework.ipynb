{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Tech Lead Homework\n",
    "\n",
    "In this document you will see a couple execises we would like you to solve. Please carefully read each task and create your solutions as you see fit.\n",
    "\n",
    "You can use any documentation for Python libraries, API endpoints, database dialects, however using any other resource to solve the coding exercises is strongly prohibited.\n",
    "\n",
    "Start with the prerequisites!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You should already have Python 3.11, Docker and a Database IDE (e.g. DataGrip, DBeaver) installed. Please make sure that the Docker daemon is running.\n",
    "\n",
    "Open a terminal in VS Code (MacOS: `Cmd + Shift + P` -> Toggle Terminal, Windows: `Ctrl + Shift + P` -> Toggle Terminal) and type the command below.\n",
    "\n",
    "‚ùóÔ∏è You should keep using the Terminal in VS Code during your homework, to make sure everything is working as intended.\n",
    "\n",
    "```bash\n",
    "docker ps\n",
    "```\n",
    "\n",
    "If Docker is installed, but not running, you will see the following error message and you should start Docker Desktop:\n",
    "\n",
    "`Cannot connect to the Docker daemon at unix:///Users/<username>/.docker/run/docker.sock. Is the docker daemon running?`\n",
    "\n",
    "Otherwise you will see the headers of containers, e.g.:\n",
    "\n",
    "`CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES`\n",
    "\n",
    "\n",
    "Create a virtual environment for Python, activate it, and install the packages outlined in requirements.txt:\n",
    "\n",
    "```bash\n",
    "python -m venv .venv\n",
    "\n",
    "source .venv/bin/activate\n",
    "\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "You should select the virtual environment created as your Python kernel for this notebook in the top right corner.\n",
    "\n",
    "Some of the most common libraries are already included in `requirements.txt`, however if at any point of this homework you need to install an additional library to solve an exercise, you can install it with `pip` (while the venv is activated).\n",
    "\n",
    "Start the Postgres Server that is needed throughout your homework:\n",
    "```bash\n",
    "cd scripts\n",
    "python -m run_postgres_server\n",
    "```\n",
    "\n",
    "Now everything is set up, let's start with the exercises!\n",
    "\n",
    "In the database you will see tables with relevant information about movies, provided by TMDb.\n",
    "\n",
    "‚ùóÔ∏è If at any point you have trouble with an exercise, you can freely move on, they are not dependent on each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You should already have Python 3.11, Docker and a Database IDE (e.g. DataGrip, DBeaver) installed. Please make sure that the Docker daemon is running.\n",
    "\n",
    "Open a terminal in VS Code (MacOS: `Cmd + Shift + P` -> Toggle Terminal, Windows: `Ctrl + Shift + P` -> Toggle Terminal) and type the command below.\n",
    "\n",
    "‚ùóÔ∏è You should keep using the Terminal in VS Code during your homework, to make sure everything is working as intended.\n",
    "\n",
    "```bash\n",
    "docker ps\n",
    "```\n",
    "\n",
    "If Docker is installed, but not running, you will see the following error message and you should start Docker Desktop:\n",
    "\n",
    "`Cannot connect to the Docker daemon at unix:///Users/<username>/.docker/run/docker.sock. Is the docker daemon running?`\n",
    "\n",
    "Otherwise you will see the headers of containers, e.g.:\n",
    "\n",
    "`CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES`\n",
    "\n",
    "\n",
    "Create a virtual environment for Python, activate it, and install the packages outlined in requirements.txt:\n",
    "\n",
    "```bash\n",
    "python -m venv .venv\n",
    "\n",
    "source .venv/bin/activate\n",
    "\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "You should select the virtual environment created as your Python kernel for this notebook in the top right corner.\n",
    "\n",
    "Some of the most common libraries are already included in `requirements.txt`, however if at any point of this homework you need to install an additional library to solve an exercise, you can install it with `pip` (while the venv is activated).\n",
    "\n",
    "Start the Postgres Server that is needed throughout your homework:\n",
    "```bash\n",
    "cd scripts\n",
    "python -m run_postgres_server\n",
    "```\n",
    "\n",
    "Now everything is set up, let's start with the exercises!\n",
    "\n",
    "In the database you will see tables with relevant information about movies, provided by TMDb.\n",
    "\n",
    "‚ùóÔ∏è If at any point you have trouble with an exercise, you can freely move on, they are not dependent on each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 (0.5 point)\n",
    "\n",
    "Write an SQL query that returns the number of films produced in Hungary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT country_name, COUNT(distinct(movie_id)) as Number_Movie\n",
    "FROM staging.production_countries p inner join staging.movie_production_countries mp on p.country_iso = mp.country_iso \n",
    "WHERE country_name = 'Hungary'\n",
    "group by country_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 (0.5 point)\n",
    "\n",
    "Write an SQL query that returns the title and release year of movies that grossed over 1 billion USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT m.title, EXTRACT(YEAR FROM m.release_date::date) AS release_year\n",
    "FROM staging.movies m INNER JOIN staging.financials f ON m.id = f.movie_id\n",
    "WHERE f.revenue > 1000000000;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 (1 point)\n",
    "\n",
    "Write an SQL query that returns the top 3 movies that achieved the most revenue by country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT country_name, title, revenue\n",
    "FROM (\n",
    "    SELECT m.title, f.revenue,p.country_name,\n",
    "           ROW_NUMBER() OVER (PARTITION BY c.country_iso ORDER BY f.revenue DESC) AS rank\n",
    "    FROM staging.movies m\n",
    "    INNER JOIN staging.financials f ON m.id = f.movie_id\n",
    "    inner join staging.movie_production_countries c on m.id = c.movie_id\n",
    "    inner join staging.production_countries p on c.country_iso = p.country_iso\n",
    ") ranked_movies\n",
    "WHERE rank <= 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 (1 point)\n",
    "\n",
    "Write an SQL query that returns the top 5 countries based on the average rating of movies produced there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT country_name, AVG(v.vote_average) AS average_rating\n",
    "FROM staging.votes v\n",
    "INNER JOIN staging.movie_production_countries c ON v.movie_id = c.movie_id\n",
    "inner join staging.production_countries p on c.country_iso = p.country_iso\n",
    "GROUP BY country_name\n",
    "HAVING AVG(v.vote_average) IS NOT NULL\n",
    "ORDER BY average_rating DESC\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 (1 point)\n",
    "\n",
    "It seems like data in `staging.production_companies` table is duplicated. Write an SQL query that fixes it without risking any data that is currently in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WITH deduplicated_companies AS (\n",
    "    SELECT\n",
    "        company_id,\n",
    "        company_name,\n",
    "        ROW_NUMBER() OVER (PARTITION BY company_id, company_name ORDER BY company_id) AS row_num\n",
    "    FROM staging.production_companies\n",
    ")\n",
    "SELECT\n",
    "    company_id,\n",
    "    company_name\n",
    "FROM\n",
    "    deduplicated_companies\n",
    "WHERE\n",
    "    row_num = 1;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would have prevented this duplication to occur? How would you fix it? (0.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using INSERT ... ON CONFLICT  duplicate records are handled gracefully by either ignoring them or updating the existing row in PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 (3 points)\n",
    "\n",
    "It seems like that the data is not full in the tables. You see a couple of `.csv` files with the prefix of `staging` in the data folder.\n",
    "\n",
    "Load these files to the DB. Make sure that rows would not be duplicated.\n",
    "\n",
    "ü©º *You can find the SQLAlchemy connection URL in your terminal window after you started the Postgres server.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data insertion process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# Replace with your actual connection string\n",
    "connection_string = 'postgresql+psycopg2://de_homework_user:u3Hg3tOy59uG@localhost:58563/homework_db'\n",
    "engine = create_engine(connection_string, pool_pre_ping=True)\n",
    "\n",
    "# Function to load a CSV into a DataFrame and remove duplicates\n",
    "def load_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n",
    "\n",
    "# Function to convert NumPy types to native Python types\n",
    "def convert_to_native_type(value):\n",
    "    if pd.isnull(value):\n",
    "        return None\n",
    "    if isinstance(value, (pd.Int64Dtype, pd.Float64Dtype, pd.StringDtype)):\n",
    "        return value\n",
    "    if hasattr(value, 'item'):\n",
    "        return value.item()  # Convert numpy types to native types\n",
    "    return value\n",
    "\n",
    "# Function to insert data into the database\n",
    "def insert_data(engine, table_name, schema_name, dataframe, insert_query, params_mapping):\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in dataframe.iterrows():\n",
    "            try:\n",
    "                # Convert row values to native Python types\n",
    "                params = {key: convert_to_native_type(row.get(key)) for key in params_mapping}\n",
    "                conn.execute(insert_query, params)\n",
    "            except SQLAlchemyError as e:\n",
    "                print(f\"Error inserting row into {schema_name}.{table_name}: {e}\")\n",
    "\n",
    "# Data configurations\n",
    "filepath = 'C:/Users/omogn/Documents/HomeWORK/data-tech-lead-homework/data/'\n",
    "data_configs = [\n",
    "    {\n",
    "        'file': f\"{filepath}/staging_movies.csv\",\n",
    "        'table': 'movies',\n",
    "        'schema': 'staging',\n",
    "        'insert_query': text(\"\"\"\n",
    "            INSERT INTO staging.movies (id, title, original_title, status, release_date, runtime, original_language, homepage)\n",
    "            VALUES (:id, :title, :original_title, :status, :release_date, :runtime, :original_language, :homepage)\n",
    "            ON CONFLICT (id) DO NOTHING;\n",
    "        \"\"\"),\n",
    "        'params': ['id', 'title', 'original_title', 'status', 'release_date', 'runtime', 'original_language', 'homepage']\n",
    "    },\n",
    "    {\n",
    "        'file': f\"{filepath}/staging_financials.csv\",\n",
    "        'table': 'financials',\n",
    "        'schema': 'staging',\n",
    "        'insert_query': text(\"\"\"\n",
    "            INSERT INTO staging.financials (movie_id, budget, revenue)\n",
    "            VALUES (:movie_id, :budget, :revenue)\n",
    "            ON CONFLICT (movie_id) DO NOTHING;\n",
    "        \"\"\"),\n",
    "        'params': ['movie_id', 'budget', 'revenue']\n",
    "    },\n",
    "    {\n",
    "        'file': f\"{filepath}/staging_movie_genres.csv\",\n",
    "        'table': 'movie_genres',\n",
    "        'schema': 'staging',\n",
    "        'insert_query': text(\"\"\"\n",
    "            INSERT INTO staging.movie_genres (movie_id, genre_id)\n",
    "            VALUES (:movie_id, :genre_id)\n",
    "            ON CONFLICT (movie_id, genre_id) DO NOTHING;\n",
    "        \"\"\"),\n",
    "        'params': ['movie_id', 'genre_id']\n",
    "    },\n",
    "    {\n",
    "        'file': f\"{filepath}/staging_movie_cast.csv\",\n",
    "        'table': 'movie_cast',\n",
    "        'schema': 'staging',\n",
    "        'insert_query': text(\"\"\"\n",
    "            INSERT INTO staging.movie_cast (movie_id, credit_id, cast_id, character_id)\n",
    "            VALUES (:movie_id, :credit_id, :cast_id, :character_id)\n",
    "            ON CONFLICT (movie_id, credit_id) DO NOTHING;\n",
    "        \"\"\"),\n",
    "        'params': ['movie_id', 'credit_id', 'cast_id', 'character_id']\n",
    "    },\n",
    "    {\n",
    "        'file': f\"{filepath}/staging_votes.csv\",\n",
    "        'table': 'votes',\n",
    "        'schema': 'staging',\n",
    "        'insert_query': text(\"\"\"\n",
    "            INSERT INTO staging.votes (movie_id, vote_average, vote_count)\n",
    "            VALUES (:movie_id, :vote_average, :vote_count)\n",
    "            ON CONFLICT (movie_id) DO NOTHING;\n",
    "        \"\"\"),\n",
    "        'params': ['movie_id', 'vote_average', 'vote_count']\n",
    "    },\n",
    "    {\n",
    "        'file': f\"{filepath}/staging_movie_crew.csv\",\n",
    "        'table': 'movie_crew',\n",
    "        'schema': 'staging',\n",
    "        'insert_query': text(\"\"\"\n",
    "            INSERT INTO staging.movie_crew (movie_id, credit_id, crew_id, job_id)\n",
    "            VALUES (:movie_id, :credit_id, :crew_id, :job_id)\n",
    "            ON CONFLICT (movie_id, credit_id) DO NOTHING;\n",
    "        \"\"\"),\n",
    "        'params': ['movie_id', 'credit_id', 'crew_id', 'job_id']\n",
    "    },\n",
    "    {\n",
    "        'file': f\"{filepath}/staging_movie_production_companies.csv\",\n",
    "        'table': 'movie_production_companies',\n",
    "        'schema': 'staging',\n",
    "        'insert_query': text(\"\"\"\n",
    "            INSERT INTO staging.movie_production_companies (movie_id, company_id)\n",
    "            VALUES (:movie_id, :company_id)\n",
    "            ON CONFLICT (movie_id, company_id) DO NOTHING;\n",
    "        \"\"\"),\n",
    "        'params': ['movie_id', 'company_id']\n",
    "    },\n",
    "    {\n",
    "        'file': f\"{filepath}/staging_movie_spoken_languages.csv\",\n",
    "        'table': 'movie_spoken_languages',\n",
    "        'schema': 'staging',\n",
    "        'insert_query': text(\"\"\"\n",
    "            INSERT INTO staging.movie_spoken_languages (movie_id, language_iso)\n",
    "            VALUES (:movie_id, :language_iso)\n",
    "            ON CONFLICT (movie_id, language_iso) DO NOTHING;\n",
    "        \"\"\"),\n",
    "        'params': ['movie_id', 'language_iso']\n",
    "    },\n",
    "    {\n",
    "        'file': f\"{filepath}/staging_movie_production_countries.csv\",\n",
    "        'table': 'movie_production_countries',\n",
    "        'schema': 'staging',\n",
    "        'insert_query': text(\"\"\"\n",
    "            INSERT INTO staging.movie_production_countries (movie_id, country_iso)\n",
    "            VALUES (:movie_id, :country_iso)\n",
    "            ON CONFLICT (movie_id, country_iso) DO NOTHING;\n",
    "        \"\"\"),\n",
    "        'params': ['movie_id', 'country_iso']\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Main process to load and insert data\n",
    "for config in data_configs:\n",
    "    df = load_csv(config['file'])\n",
    "    insert_data(engine, config['table'], config['schema'], df, config['insert_query'], config['params'])\n",
    "\n",
    "print(\"Data insertion process completed successfully.\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 (3 points)\n",
    "\n",
    "It seems like that in the `staging.votes` table we have a couple of fields with `NULL` values.\n",
    "\n",
    "Use the TMDb API to retrieve the `vote_count` and `vote_average` fields for these movies and update the table.\n",
    "\n",
    "TMDb documentation is available here: https://developer.themoviedb.org/reference/intro/getting-started\n",
    "\n",
    "You can use this header for authentication:\n",
    "```python\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJjMDdiMGIwMmQwNzM4ZmU3YWVmNzEyMDk3OTlkOWExNCIsInN1YiI6IjY2NGM3YjM3OThmNWZjNWJiNTU1MmNlNiIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.oN61KwLmLTLir5KML1pzZ7raPn4iDZREOI59XT5cyIk\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Replace with your actual connection string\n",
    "connection_string = 'postgresql+psycopg2://de_homework_user:jokcoG44hoKW@localhost:51409/homework_db'\n",
    "engine = create_engine(connection_string,pool_pre_ping=True)\n",
    "conn = engine.connect()\n",
    "\n",
    "# Define your headers with authentication\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJjMDdiMGIwMmQwNzM4ZmU3YWVmNzEyMDk3OTlkOWExNCIsInN1YiI6IjY2NGM3YjM3OThmNWZjNWJiNTU1MmNlNiIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.oN61KwLmLTLir5KML1pzZ7raPn4iDZREOI59XT5cyIk\"\n",
    "}\n",
    "\n",
    "# SQL query to get movie IDs with NULL vote_count or vote_average\n",
    "query = \"SELECT movie_id FROM staging.votes WHERE vote_count IS NULL OR vote_average IS NULL;\"\n",
    "\n",
    "# Fetch movie IDs from the database\n",
    "with conn:\n",
    "    result = conn.execute(text(query))\n",
    "    movie_ids = [row['movie_id'] for row in result]\n",
    "\n",
    "# Function to get vote details from TMDb API\n",
    "def fetch_vote_details(movie_id):\n",
    "    try:\n",
    "        url = f\"https://api.themoviedb.org/3/movie/{movie_id}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return {\n",
    "                \"movie_id\": movie_id,\n",
    "                \"vote_count\": data.get(\"vote_count\"),\n",
    "                \"vote_average\": data.get(\"vote_average\")\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for movie_id: {movie_id}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fetch details for all movies with missing votes\n",
    "movie_details_list = []\n",
    "for movie_id in movie_ids:\n",
    "    details = fetch_vote_details(movie_id)\n",
    "    if details:\n",
    "        movie_details_list.append(details)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(movie_details_list)\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Function to update vote data in the database\n",
    "def update_vote_data(movie_id, vote_count, vote_average):\n",
    "    update_query = text(\"\"\"\n",
    "        UPDATE staging.votes\n",
    "        SET vote_count = :vote_count, vote_average = :vote_average\n",
    "        WHERE movie_id = :movie_id;\n",
    "    \"\"\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(update_query, {\n",
    "            'movie_id': movie_id,\n",
    "            'vote_count': vote_count,\n",
    "            'vote_average': vote_average\n",
    "        })\n",
    "\n",
    "# Update each movie record\n",
    "for _, row in df.iterrows():\n",
    "    if pd.notnull(row['vote_count']) and pd.notnull(row['vote_average']):\n",
    "        update_vote_data(row['movie_id'], row['vote_count'], row['vote_average'])\n",
    "\n",
    "print(\"Database updated successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8 (3 points)\n",
    "\n",
    "Write an SQL query that returns the title of all of the space colony movies.\n",
    "\n",
    "We do not have the `keywords` column in tabular format from `raw.movies`. You can query that one or create a new table in the `staging` schema that contains this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## query to return all title space colony movies\n",
    "SELECT movie_id,title\n",
    "FROM (SELECT\n",
    "    jsonb_array_elements(keywords) ->> 'id' AS movie_id,\n",
    "    jsonb_array_elements(keywords) ->> 'name' AS title\n",
    "FROM raw.movies)\n",
    "where title ='space colony'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9 (6 points)\n",
    "\n",
    "Your task is to design a datamart on top of the staging tables. Your client is building a BI dashboard and is looking to answer the following KPIs:\n",
    "- correlation between revenue and ratings\n",
    "- top 10 movies by revenue\n",
    "- number of movies produced by each production company\n",
    "- average budget and revenue by genre\n",
    "- most prevalent jobs in the movie industry\n",
    "\n",
    "Create an ELT pipeline to load data from the staging tables to the datamart tables you designed. For bonus points, write your functions for Dagster, Airflow or Prefect frameworks.\n",
    "\n",
    "Your schema should be called `datamart`. You are free to design the datamart however you see fit, the main goal is to be able to answer these questions based on the datamart on a BI dashboard.\n",
    "\n",
    "‚ùó *You do not need to implement a full solution, just design your DAGs with these frameworks (using decorators, IO managers, etc) in mind for the bonus points.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dagster import job, op, In, Out\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "\n",
    "# Replace with your actual connection string\n",
    "connection_string = 'postgresql+psycopg2://de_homework_user:jokcoG44hoKW@localhost:51409/homework_db'\n",
    "engine = create_engine(connection_string,pool_pre_ping=True)\n",
    "conn = engine.connect()\n",
    "\n",
    "# Extract Operations\n",
    "@op(out={\"movies\": Out(), \"production_companies\": Out(), \"movie_genres\": Out(), \"financials\": Out(), \"votes\": Out(),\"movie_production_companies\": Out(),\"genres\": Out(),\"movie_crew\": Out(),\"crew_job\": Out()})\n",
    "def extract_data():\n",
    "    with engine.connect() as conn:\n",
    "        movies = pd.read_sql(\"SELECT * FROM staging.movies\", conn)\n",
    "        production_companies = pd.read_sql(\"SELECT * FROM staging.production_companies\", conn)\n",
    "        movie_genres = pd.read_sql(\"SELECT * FROM staging.movie_genres\", conn)\n",
    "        financials = pd.read_sql(\"SELECT * FROM staging.financials\", conn)\n",
    "        votes = pd.read_sql(\"SELECT * FROM staging.votes\", conn)\n",
    "        movie_production_companies = pd.read_sql(\"SELECT * FROM staging.movie_production_companies\", conn)\n",
    "        genres = pd.read_sql(\"SELECT * FROM staging.genres\", conn)\n",
    "        movie_crew = pd.read_sql(\"SELECT * FROM staging.movie_crew\", conn)\n",
    "        crew_job = pd.read_sql(\"SELECT * FROM staging.crew_job\", conn)\n",
    "\n",
    "\n",
    "    return movies, production_companies, movie_genres, financials, votes,movie_production_companies,genres,movie_crew,crew_job\n",
    "\n",
    "# Call the function to extract data\n",
    "movies, production_companies, movie_genres, financials, votes,movie_production_companies,genres,movie_crew,crew_job = extract_data()\n",
    "                            \n",
    "\n",
    "##Transformation of data for Movies metrics,production company summary,genre summary and job frequency\n",
    "\n",
    "@op(ins={\"movies\": In(), \"movie_genres\": In(), \"financials\": In(), \"votes\": In()})\n",
    "def transform_movie_metrics(movies, movie_genres, financials, votes):\n",
    "    # Ensure 'movie_id' exists in financials, movie_genres, and votes\n",
    "    # Check for common columns that can be used for joining\n",
    "    # Rename columns where necessary to avoid conflicts after merging\n",
    "    \n",
    "    # Merge movies with financials\n",
    "    merged_df = (\n",
    "        movies.merge(movie_genres, left_on='id', right_on='movie_id', how='left')\n",
    "              .merge(financials, left_on='id', right_on='movie_id', how='left')\n",
    "              .merge(votes, left_on='id', right_on='movie_id', how='left')\n",
    "    )\n",
    "\n",
    "    # Select and rename columns as needed to avoid duplicates\n",
    "    movie_metrics = merged_df[['id','movie_id','title', 'genre_id', 'status', 'revenue', 'budget', 'vote_average', 'vote_count']].drop_duplicates()\n",
    "    return movie_metrics\n",
    "\n",
    "# Example: assuming these variables are defined correctly\n",
    "movie_metrics = transform_movie_metrics(movies, movie_genres, financials, votes)\n",
    "\n",
    "# Print the result\n",
    "#print(movie_metrics.head())\n",
    "\n",
    "\n",
    "@op(ins={\"movie_production_companies\": In(), \"production_companies\": In()})\n",
    "def transform_production_company_summary(production_companies, movie_production_companies):\n",
    "    # Merge the two DataFrames on `company_id`\n",
    "    merged_df = (movie_production_companies.merge(production_companies, on='company_id', how='inner')\n",
    "                                          .merge(financials, on='movie_id', how='inner'))\n",
    "    \n",
    "    \n",
    "     # Select relevant columns and drop duplicates\n",
    "    production_company_summary= merged_df[['company_id', 'company_name', 'movie_id', 'revenue', 'budget']].drop_duplicates()\n",
    "    \n",
    "    \n",
    "    # Group by `genre_id` and `genre_name`, and calculate required metrics\n",
    "    production_company_summary = (\n",
    "        production_company_summary.groupby(['company_id', 'company_name'])\n",
    "        .agg(\n",
    "            movie_count=('movie_id', 'count'), \n",
    "            total_revenue=('revenue', 'mean'), \n",
    "            total_budget=('budget', 'mean')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Sort by `vol` in descending order\n",
    "    #result = result.sort_values(by='vol', ascending=False)\n",
    "    \n",
    "    return production_company_summary\n",
    "\n",
    "# Call the function with the appropriate inputs\n",
    "production_company_summary = transform_production_company_summary(production_companies, movie_production_companies)\n",
    "\n",
    "# Print or display the result\n",
    "#print(production_company_summary)\n",
    "\n",
    "\n",
    "@op(ins={\"movie_genres\": In(), \"financials\": In(), \"genres\": In()})\n",
    "def transform_production_genre_summary(movie_genres, financials, genres):\n",
    "    # Merge movie_genres with genres and financials\n",
    "    merged_df = (\n",
    "        movie_genres.merge(genres, on='genre_id', how='inner')\n",
    "                    .merge(financials, on='movie_id', how='inner')\n",
    "    )\n",
    "    \n",
    "    # Select relevant columns and drop duplicates\n",
    "    genre_summary = merged_df[['genre_id', 'genre_name', 'movie_id', 'revenue', 'budget']].drop_duplicates()\n",
    "\n",
    "    # Group by `genre_id` and `genre_name`, and calculate required metrics\n",
    "    genre_summary = (\n",
    "        genre_summary.groupby(['genre_id', 'genre_name'])\n",
    "        .agg(\n",
    "            average_revenue=('revenue', 'mean'), \n",
    "            average_budget=('budget', 'mean'),\n",
    "            movie_count=('movie_id', 'count'),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Sort by `vol` in descending order\n",
    "    #genre_summary = genre_summary.sort_values(by='vol', ascending=False)\n",
    "    \n",
    "    return genre_summary\n",
    "\n",
    "# Call the function with the appropriate inputs\n",
    "genre_summary = transform_production_genre_summary(movie_genres, financials, genres)\n",
    "\n",
    "# Print or display the result\n",
    "#print(genre_summary)\n",
    "\n",
    "\n",
    "@op(ins={\"movie_crew\": In(), \"crew_job\": In()})\n",
    "def transform_job_frequency(movie_crew, crew_job):\n",
    "    # Merge movie_crew with crew_job on 'job_id'\n",
    "    merged_df = movie_crew.merge(crew_job, on='job_id', how='inner')\n",
    "    \n",
    "    # Select relevant columns and drop duplicates\n",
    "    job_frequency = merged_df[['job_id', 'job_name', 'movie_id']].drop_duplicates()\n",
    "\n",
    "    # Group by `job_id` and `job_name`, and calculate the occurrence count\n",
    "    job_frequency = (\n",
    "        job_frequency.groupby(['job_id', 'job_name'])\n",
    "        .agg(\n",
    "            occurrence_count=('movie_id', 'count')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Return the resulting DataFrame\n",
    "    return job_frequency\n",
    "\n",
    "# Call the function with the appropriate inputs\n",
    "job_frequency = transform_job_frequency(movie_crew, crew_job)\n",
    "\n",
    "# Print or display the result\n",
    "#print(job_frequency)\n",
    "\n",
    "\n",
    "## Load Operations movie metric\n",
    "\n",
    "@op(ins={\"movie_metrics\": In()})\n",
    "def load_movie_metrics(movie_metrics):\n",
    "    #with engine.connect() as conn:\n",
    "        movie_metrics.to_sql(\"movie_metrics\", conn, schema=\"datamart\", if_exists=\"replace\", index=False)\n",
    "        return movie_metrics   \n",
    "\n",
    "movie_metrics = load_movie_metrics(movie_metrics)\n",
    "\n",
    "#print(movie_metrics.head())\n",
    "\n",
    "\n",
    "\n",
    "## Load Operations production company summary\n",
    "\n",
    "@op(ins={\"production_company_summary\": In()})\n",
    "def load_production_company_summary(production_company_summary):\n",
    "    #with engine.connect() as conn:\n",
    "        production_company_summary.to_sql(\"production_company_summary\", conn, schema=\"datamart\", if_exists=\"replace\", index=False)\n",
    "        return production_company_summary   \n",
    "\n",
    "production_company_summary = load_production_company_summary(production_company_summary)\n",
    "\n",
    "#print(production_company_summary.head())\n",
    "\n",
    "\n",
    "## Load Operations genre summary\n",
    "\n",
    "@op(ins={\"genre_summary\": In()})\n",
    "def load_genre_summary(genre_summary):\n",
    "    #with engine.connect() as conn:\n",
    "        genre_summary.to_sql(\"genre_summary\", conn, schema=\"datamart\", if_exists=\"replace\", index=False)\n",
    "        return genre_summary   \n",
    "\n",
    "genre_summary = load_genre_summary(genre_summary)\n",
    "\n",
    "#print(genre_summary.head())\n",
    "\n",
    " \n",
    "## Load Operations job frequency\n",
    "\n",
    "@op(ins={\"job_frequency\": In()})\n",
    "def load_job_frequency(job_frequency):\n",
    "    #with engine.connect() as conn:\n",
    "        job_frequency.to_sql(\"job_frequency\", conn, schema=\"datamart\", if_exists=\"replace\", index=False)\n",
    "        return job_frequency   \n",
    "\n",
    "job_frequency = load_job_frequency(job_frequency)\n",
    "\n",
    "#print(job_frequency.head())\n",
    "\n",
    "\n",
    "\n",
    "## Pipeline to load the the data \n",
    "\n",
    "@job\n",
    "def datamart_elt_pipeline():\n",
    "    movies, production_companies, movie_genres, financials, votes,movie_production_companies,genres,movie_crew,crew_job = extract_data()\n",
    "    movie_metrics = transform_movie_metrics(movies, movie_genres, financials, votes)\n",
    "    movie_metrics = load_movie_metrics(movie_metrics)\n",
    "                                       \n",
    "    production_company_summary = transform_production_genre_summary(movie_production_companies,financials,production_companies)                                 \n",
    "    production_company_summary = load_production_company_summary(production_company_summary)\n",
    "                                       \n",
    "    genre_summary = transform_production_genre_summary(movie_genres, financials, genres)                                   \n",
    "    genre_summary = load_genre_summary(genre_summary)\n",
    "                                       \n",
    "    job_frequency = transform_job_frequency(movie_crew, crew_job)                                  \n",
    "    job_frequency = load_job_frequency(job_frequency)    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10 (10 points)\n",
    "\n",
    "In this assignment we will describe an imaginary project about a US based home insurance company who would like to have a plan for an independent centralised reporting platform.\n",
    "Your task will be to evaluate the situation and come-up with a plan to deliver it.\n",
    "\n",
    "#### WBSS Home Insurance Overview\n",
    "\n",
    "There is a US based home insurance company who works as an insurance platform. Their USP is that they are acting as a one-stop-shop and provide not just insurance policy handling but water leakage sensors to their clients.\n",
    "The platform is a website where the clients can login, look and manage their policy statuses, they can enroll to the smart protection program, they can submit claims, and receive alerts when there is a water leakage in the house.\n",
    "The insurance company works with different 3rd parties for:\n",
    "- policy management\n",
    "- sensor shipment and sensor data collection\n",
    "- customer claim handling\n",
    "\n",
    "The request is to provide a plan to have an independent centralised business reporting platform under their own premises.\n",
    "\n",
    "#### Source Systems and Architecture\n",
    "\n",
    "The WBSS platform itself is a website (built on Azure) where there is a Google Analytics event tracking in place to see platform performance and customer behaviour.\n",
    "The 3rd party policy management system can provide CSV file extracts on a daily basis about the policy status changes and cover new policies.\n",
    "The smart home sensor 3rd party vendor is able to share smart home data (customer, event and sensor level information) through providing an AWS RDS server view access.\n",
    "The claim handling company provided a SOAP API endpoint with access to request data for all needed claims.\n",
    "The logical relationship between the data points is the following: An insurance holder can have a policy related to a home. In the home there can be installed smart home sensors. These sensors are sending health and alert data that can become a claim in case of a damage. Also, they can submit individual claims through the WBSS platform that is appearing in the claims management system as well.\n",
    "\n",
    "#### Your Task\n",
    "- Create a high level architecture plan in Azure where it is possible to integrate all data sources and have a connected datamart. Describe data ingestion for all relevant sources, plan the ETL/ELT pipelines, describe what components you would use and what tools.\n",
    "- Create a high level estimate for the main tasks.\n",
    "- List possible risks, and desrcibe how would you mitigate them.\n",
    "- How would you start to estimate and optimize cloud costs? Please let us know your thought process.\n",
    "- You can use any (free) tool for your architecture design (e.g. Draw.io, Eraser) and please include a picture of it below. For the estimations and risks, you can use the markdown cell below.\n",
    "- Be as thorough as possible, if you feel you need to add additional notes to describe your solution, please do so.\n",
    "- Where you feel you don't have detailed enough information, make your own assumptions and recommendations.\n",
    "\n",
    "\n",
    "### Exercise 10 Solution:\n",
    "High-Level Architecture Plan in Azure\n",
    "\n",
    "To create a centralized business reporting platform, we will leverage the Modern Analytics Architecture on Azure with Azure Databricks as the core engine for data processing and transformation. This architecture will provide a robust, scalable, and efficient system to integrate all data sources and deliver a connected datamart for business intelligence and reporting.\n",
    "\n",
    "Architecture Overview\n",
    "This architecture uses a Medallion Data Lake Architecture (Bronze, Silver, Gold) to manage data at different stages:\n",
    "‚Ä¢\tBronze Layer: Raw data directly from source systems, stored for archival purposes.\n",
    "‚Ä¢\tSilver Layer: Cleaned, deduplicated, and normalized data.\n",
    "‚Ä¢\tGold Layer: Aggregated, analytics-ready data used for business intelligence.\n",
    "\n",
    "For Architecture design diagram:Please refer to git repository. \n",
    "\n",
    "The key components of this architecture include:\n",
    "\n",
    "1. Azure Data Lake Storage (ADLS) Gen2\n",
    "‚Ä¢\tPurpose: Central repository for all data, with hierarchical structure.\n",
    "‚Ä¢\tFunctionality:\n",
    "o\tBronze Layer: Stores raw data ingested from external sources, including CSV files, sensor data from AWS RDS, and claims data from SOAP API.\n",
    "o\tSilver Layer: Stores cleaned and structured data after transformation.\n",
    "o\tGold Layer: Stores aggregated and enriched data ready for analytics and reporting.\n",
    "\n",
    "2. Azure Data Factory (ADF)\n",
    "‚Ä¢\tPurpose: Ingest data from various sources and orchestrate data pipelines.\n",
    "‚Ä¢\tFunctionality:\n",
    "o\tCSV Files: Connect to policy management system, import CSV extracts into the Bronze Layer of ADLS.\n",
    "o\tAWS RDS Data: Use an integrated connector to extract sensor data from AWS databases.\n",
    "o\tSOAP API: Interact with claims handling system to request data using REST/SOAP activities.\n",
    "o\tScheduled Pipelines: Set up pipelines to run at specific intervals (e.g., daily for batch processing or near real-time for sensor alerts).\n",
    "\n",
    "3. Azure Databricks\n",
    "‚Ä¢\tPurpose: Central engine for data processing, transformation, and advanced analytics.\n",
    "‚Ä¢\tFunctionality:\n",
    "o\tETL/ELT Processes: Transform raw data (from Bronze to Silver) by cleansing, deduplication, and applying business rules.\n",
    "o\tAggregation: Prepare data (from Silver to Gold) by performing data joins, aggregations, and enrichment to create ready-to-use datasets for reporting.\n",
    "o\tReal-Time Streaming: Ingest real-time sensor data using Structured Streaming to capture alerts and events.\n",
    "\n",
    "4. Azure Synapse Analytics\n",
    "‚Ä¢\tPurpose: Serve as the central data warehouse, housing the final, processed data from the Gold Layer.\n",
    "‚Ä¢\tFunctionality:\n",
    "o\tFast Querying: Optimized for handling large-scale queries and supporting business intelligence requirements.\n",
    "o\tData Modeling: Define fact and dimension tables to support relational queries and Power BI reports.\n",
    "o\tData Exploration: SQL-based exploration for ad-hoc analysis.\n",
    "\n",
    "5. Power BI\n",
    "‚Ä¢\tPurpose: Visualization and business intelligence tool for creating dashboards and reports.\n",
    "‚Ä¢\tFunctionality:\n",
    "o\tReal-Time Dashboards: Create dynamic dashboards that reflect real-time alerts from sensors.\n",
    "o\tData Insights: Offer insights on policy trends, claim patterns, customer behavior, and platform performance.\n",
    "o\tUser Access Control: Configure access permissions for different teams (e.g., claims team, management, operations).\n",
    "\n",
    "6. Azure Key Vault\n",
    "‚Ä¢\tPurpose: Secure storage for sensitive information like API keys, database connection strings, and other secrets.\n",
    "‚Ä¢\tFunctionality:\n",
    "o\tData Security: Ensure secure data access, enhancing the platform‚Äôs security posture.\n",
    "o\tAccess Management: Control access to keys, secrets, and certificates to prevent unauthorized data access.\n",
    "\n",
    "7. Azure Monitor & Log Analytics\n",
    "‚Ä¢\tPurpose: Monitoring, alerting, and diagnostics for the entire data platform.\n",
    "‚Ä¢\tFunctionality:\n",
    "o\tMonitoring Pipelines: Track data pipeline execution, detect failures, and alert teams for immediate action.\n",
    "o\tResource Utilization: Monitor CPU, memory, and storage utilization across Databricks, ADF, and Synapse to ensure optimal performance.\n",
    "o\tDiagnostic Logs: Collect logs and diagnostic data to troubleshoot any issues.\n",
    "\n",
    "Detailed Data Ingestion and ETL/ELT Pipelines\n",
    "\n",
    "Data Sources & Ingestion\n",
    "\n",
    "Data Source              \t          Ingestion Method\t               Frequency\t           Storage Layer\n",
    "Policy Management (CSV)\tADF           File Ingestion\t               Daily Batch             ADLS (Bronze)\n",
    "Smart Home Sensors (AWS RDS)\t      ADF Database Connector\t       Near Real-Time\t       ADLS (Bronze)\n",
    "Claims Handling (SOAP API)\t          Azure Logic Apps + ADF\t       Daily Batch\t           ADLS (Bronze)\n",
    "Google Analytics (User Behavior)\t  Google Analytics API Connector   Hourly\t               ADLS (Bronze)\n",
    "\n",
    "\n",
    "ETL/ELT Pipelines\n",
    "1.\tBronze to Silver Transformation (Data Cleansing)\n",
    "o\tTask: Deduplicate records, handle missing values, normalize data, validate data types.\n",
    "o\tTool: Azure Databricks\n",
    "o\tOutput: Clean and structured data stored in the Silver Layer of ADLS.\n",
    "\n",
    "2.\tSilver to Gold Transformation (Data Aggregation & Enrichment)\n",
    "o\tTask: Join datasets (e.g., linking policy data with sensor data), aggregate information (e.g., total claims per policy), enrich with calculated fields (e.g., risk scores).\n",
    "o\tTool: Azure Databricks\n",
    "o\tOutput: Aggregated datasets stored in Azure Synapse Analytics for fast querying.\n",
    "\n",
    "3.\tReal-Time Processing (Sensor Alerts)\n",
    "o\tTask: Stream sensor alerts and events, process them in real-time to detect anomalies (e.g., water leakage alerts).\n",
    "o\tTool: Azure Databricks Structured Streaming\n",
    "o\tOutput: Alert data stored in ADLS (Silver) and pushed to Azure Synapse Analytics (Gold) for live monitoring via Power BI.\n",
    "\n",
    "\n",
    "3. High-Level Task Estimation\n",
    "Task\t                                         Description\t                                                   Estimated  Time\n",
    "Architecture Design     \t              Define architecture, components, and data flow.          \t                   1 Week\n",
    "Data Ingestion Setup\t                  Configure ADF, Logic Apps, and connectors for data sources.\t               2 Weeks\n",
    "Azure Databricks ETL/ELT Pipelines\t      Develop data transformation and aggregation pipelines.\t                   5 Weeks\n",
    "Azure Synapse Setup & Data Modeling\t      Design data schemas, tables, and relationships.\t                           2 Weeks\n",
    "Power BI Dashboard Design\t              Develop and deploy reporting dashboards for various stakeholders.            3 Weeks\n",
    "Testing & Quality Assurance\t              Validate data flow, data accuracy, and system integration.       \t           2 Weeks\n",
    "Deployment & Documentation\t              Deploy production environment, create user manuals and operational guides.   1 Week\n",
    "Ongoing Project Management\t              Ongoing tasks: planning, coordination, issue tracking, documentation.   \t   Ongoing\n",
    "\n",
    "4. Risk Assessment and Mitigation Please refer to the attached  Excel spreadsheet.\n",
    "\n",
    "5. Cloud Cost Estimation and Optimization\n",
    "\n",
    "Cost Estimation Approach\n",
    "1.\tStorage:\n",
    "o\tEstimate based on the expected data volume (GBs/TBs) per day multiplied by 30 days for monthly costs in Azure Data Lake Storage.\n",
    "2.\tCompute:\n",
    "o\tEvaluate costs based on Databricks cluster sizes and runtime (per hour). Calculate based on expected daily workload and usage.\n",
    "3.\tData Orchestration:\n",
    "o\tCalculate Azure Data Factory costs based on the number of pipeline activities and frequency of runs.\n",
    "4.\tData Warehouse:\n",
    "o\tEstimate Synapse Analytics costs based on provisioned resources and query workload.\n",
    "  \n",
    "Cost Optimization Strategies\n",
    "‚Ä¢\tAuto-Scaling & Right-Sizing: Configure auto-scaling in Azure Databricks clusters and Synapse Analytics to save costs by adjusting resources based on demand.\n",
    "‚Ä¢\tData Tiering: Move less frequently accessed data to lower-cost storage tiers.\n",
    "‚Ä¢\tOptimize Pipelines: Efficiently design data pipelines to reduce runtime, saving on compute costs.\n",
    "‚Ä¢\tCost Monitoring: Set up Azure Cost Alerts and budgets to proactively monitor and control spending.\n",
    "\n",
    "Conclusion\n",
    "The proposed architecture ensures a scalable, secure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
